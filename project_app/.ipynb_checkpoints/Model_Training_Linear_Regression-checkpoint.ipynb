{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ca72e1",
   "metadata": {},
   "source": [
    "# Model Training Execution steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea1fb6",
   "metadata": {},
   "source": [
    "## 1. Input Liberies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression , Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_infaltion_factor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import shapiro , kstest, normaltest, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d5878",
   "metadata": {},
   "source": [
    "## 2. Problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf788",
   "metadata": {},
   "outputs": [],
   "source": [
    "To predict .......... by using various features\n",
    "features = [list of independent features]\n",
    "Target = output, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646ae1f",
   "metadata": {},
   "source": [
    "## 3. Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38092ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use SQL, MongoDB\n",
    "Files>> CSV files, JASON files, Excel files\n",
    "Data frame, array, series4\n",
    "\n",
    "df.head().T # use for getting the features name in transpose i.e in row label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77980d12",
   "metadata": {},
   "source": [
    "## 4,5  EDA (Exploratory Data Analysis), Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10eb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "View liberies to analyse data or features\n",
    "1. missing values>> df.isna().sum()/.mean() * 100   # if more than 25 % of null value we can think on dropping the feature\n",
    "\n",
    "2. check Datatype >> df.info()  # if dtype is float or int we can say that there are no missing values (incuded '?')\n",
    "\n",
    "3. cleaning further data feature wise\n",
    "\n",
    "4. if there are maximum unique values then such data will not impact on outcome; we can drop it ex. serial no., person name etc\n",
    "\n",
    "5. We need to change the datatype of each feature to float or int:\n",
    "    5.1 we need to replace blank values if represented by any symbol('?') to null value so that we can use replace function\n",
    "    df['col_name'].replace({'?': np.nan}, inplace = True)\n",
    "    df['col_name'] = df['col_name'].astype(float)  # it will not accept int as Dtype becoz of null value\n",
    "    \n",
    "    5.2  Replace null value by mean , median, mode\n",
    "    mean if outliers are not present\n",
    "    mediian if outliers are present  # if outliers not checked then use median only\n",
    "    mode for categorical features\n",
    "    df['col_name'] = df['col_name'].fillna(df['col_name'].median())\n",
    "    \n",
    "6. Handling categorical data:\n",
    "    6.1 How to check\n",
    "    df['col_name'].nunique()\n",
    "    df['col_name'].value_counts()\n",
    "    \n",
    "    6.2 How to replace categorical data to numerical representation\n",
    "    a. If few unique values are present(1-3) then replace it with (1,2,3)\n",
    "    df['fuel-type'].replace({'gas':1,'diesel':0},inplace = True)\n",
    "    \n",
    "    b.If there are multiple unique value than use encoding method\n",
    "    b.1 Lable Encoding\n",
    "             >> In this approach we convert each unique value in the column to a number\n",
    "    Drawback >> The major problem of lable encoding is that the random or sequential allocation of \n",
    "                numbers might misguide the module into thinking that higher number is of more importance.\n",
    "                \n",
    "    b.2 One Hot encoding\n",
    "             >> The above drawback is addressed by one hot encoding\n",
    "             >> in this, each categorical value is converted into a new column and assigned 1 or 0(True/False)\n",
    "                Value to the column\n",
    "        There are two methods:\n",
    "            1.get_dummies\n",
    "        \n",
    "            df_col_name = pd.get_dummies(df['col_name'],prefix = 'col_name' )\n",
    "            df_oneHot = pd.get_dummies(df, columnes = ['col_name'] )\n",
    "            \n",
    "            2. OneHotEncoder\n",
    "            a.  onehot_enc = OneHotEncoder(dtype = int)\n",
    "                array = onehot_enc.fit_transform(df1[['CITY']]).toarray()\n",
    "                df_city = pd.DataFrame(array,columns=onehot_enc.categories_)\n",
    "                df_city\n",
    "                \n",
    "            b.  onehot_enc = OneHotEncoder(dtype = int)\n",
    "                onehot_enc.fit(df1[['CITY']])         # Fit \n",
    "\n",
    "                onehot_enc.transform(df1[['CITY']]).toarray()\n",
    "\n",
    "# If we want to use same OneHotEncoder in new file then we need to create a pickle file\n",
    "import pickle\n",
    "\n",
    "with open(\"OneHot.pkl\",'wb') as f:\n",
    "    pickle.dump(encoding,f)\n",
    "    \n",
    "# How to use pickle file\n",
    "import pickle \n",
    "with open(\"OneHot.pkl\",'rb') as f:\n",
    "   onehot_enc = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b14f2",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b14cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "A] If Data is normally distributed then we can use 'z-score'\n",
    "    syntax:\n",
    "        z_score_values = np.abs(zscore(array2))\n",
    "        print(z_score_values)\n",
    "        outliers_index = np.where(z_score_values >= 3)[0]\n",
    "        outliers_list = array2[outliers_index]\n",
    "        outliers_list\n",
    "\n",
    "B] If Data is skewed then we can use 'IQR'   \n",
    "    syntax:\n",
    "        q1 = df['Market_dist'].quantile(0.25)\n",
    "        q2 = df['Market_dist'].quantile(0.50)\n",
    "        q3 = df['Market_dist'].quantile(0.75)\n",
    "\n",
    "        IQR = q3 - q1\n",
    "\n",
    "        upper_tail = q3 + 1.5*IQR\n",
    "        lower_tail = q1 - 1.5*IQR\n",
    "\n",
    "        median = df['Market_dist'].median()\n",
    "\n",
    "        print('q1', q1)\n",
    "        print('q2', q2)\n",
    "        print('q3', q3)\n",
    "        print(\"Median\", median)\n",
    "        print('upper_tail', upper_tail)\n",
    "        print('lower_tail', lower_tail)\n",
    "        \n",
    "*** To detect Extreme Outliers we can change 1.5 value upto 3\n",
    "\n",
    "C]  For visualization of outliers use 'Boxplot'\n",
    "    sns.boxplot(x = df['col_name'])   # boxplot depends upon IQR values(0.25,0.50,0.75)\n",
    "    \n",
    "D]  and 'scatter plot' \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177033d7",
   "metadata": {},
   "source": [
    "#### How to handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa229369",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.************* delete observation**************\n",
    "\n",
    "    df.drop('row_name', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dddd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.*************Imputation method******************\n",
    "\n",
    "    2.1 Imputation by median value:\n",
    "        # Apply median\n",
    "        median_fare = titanic_df.loc[(titanic_df['Fare'] <= upper_tail) & \n",
    "                            (titanic_df['Fare'] >= lower_tail),'Fare'].median()\n",
    "        \n",
    "        # assigning median value\n",
    "         titanic_df.loc[(titanic_df['Fare'] > upper_tail) |\n",
    "                            (titanic_df['Fare'] < lower_tail),'Fare'] = median_fare\n",
    "            \n",
    "    2.2 Imputation by mean value: \n",
    "        # apply mean \n",
    "        mean_age = diabetes_df.loc[diabetes_df['Age'] <= upper_tail,'Age'].mean()\n",
    "        \n",
    "        # assigning mean value\n",
    "        diabetes_df.loc[diabetes_df['Age'] > upper_tail,'Age'] = mean_age\n",
    "        \n",
    "    2.3 Imutaion by upper_Tail:\n",
    "        # display unpper tail values column\n",
    "        df[['Taxi_dist']].loc[df['Taxi_dist'] > upper_tail ]\n",
    "        \n",
    "        # assigning upper_tail values\n",
    "        df.loc[df['Taxi_dist'] > upper_tail, 'Taxi_dist'] = upper_tail  # series must be empty/ outliers reduce with in the range\n",
    "        \n",
    "    2.4 Imutaion by lower_Tail:\n",
    "        # display lower tail values column\n",
    "        df[['Taxi_dist']].loc[df['Taxi_dist'] < lower_tail ]\n",
    "        \n",
    "        # assigning lower_tail values\n",
    "        df.loc[df['Taxi_dist'] < lower_tail, 'Taxi_dist'] = lower_tail\n",
    "        \n",
    "    2.5  we can replace the outliers by using min, max, 0, \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd66365",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.***************Tranformation method********************\n",
    "\n",
    "## we can change values from 1 scale to another scale in transformation\n",
    "\n",
    "    3.1 log Transformation:\n",
    "        >> 1st check outliers by using boxplot\n",
    "           sns.boxplot(x = diabetes_df['Age'])\n",
    "            \n",
    "        >> Applying log on feature\n",
    "           sns.boxplot(x = np.log(diabetes_df['Age']))\n",
    "            \n",
    "        >> for antilog we can use\n",
    "           np.exp(df['col_name'])\n",
    "            \n",
    "    3.2 CubeRoot Transformation\n",
    "    \n",
    "        >> sns.boxplot(x = np.cbrt(diabetes_df['Age']))\n",
    "        \n",
    "    3.3 Square Root Transformation\n",
    "    \n",
    "        >> sns.boxplot(x = np.sqrt(diabetes_df['Age']))\n",
    "        \n",
    "    3.4 Reciprocal\n",
    "    \n",
    "        >> sns.boxplot(x = 1/ (diabetes_df['Age']))\n",
    "        \n",
    "    3.5 standardization\n",
    "    \n",
    "        >> This methos is used only for minimize the impact, actual shape will not be change\n",
    "           will give last priority for this method, becoz we can't remove all outliers.\n",
    "           # Dispaly all z_score value\n",
    "            z_score_values = zscore(diabetes_df['Age'])\n",
    "            \n",
    "           # Kdeplot by using above values\n",
    "            sns.kdeplot(z_score_values)\n",
    "            \n",
    "    3.6 Box Cox Transformation\n",
    "    \n",
    "        >> Limitatiion in boxcox : we can not use this method if values are in negative and zero.\n",
    "            $$$ from scipy.stats import boxcox$$$\n",
    "            \n",
    "            age1 = boxcox(diabetes_df['Age'])[0]\n",
    "            sns.kdeplot(age1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c7e41",
   "metadata": {},
   "source": [
    "# 6.Feature selction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74074a",
   "metadata": {},
   "source": [
    "### Assumption of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "********** Assumption of Linear Regression *********\n",
    "\n",
    "1. Linearity:\n",
    "    We need to find correlation between independent variable and dependent variable\n",
    ">>  df.corr()\n",
    "\n",
    "# visualization by using heatmap\n",
    ">>  plt.figure(figsize=(20,20))\n",
    "    sns.heatmap(boston_df.corr(),annot = True, cmap = 'rainbow')\n",
    "    \n",
    "2. No MultiColinearity:\n",
    "    (VIF) >> Variance Inflation Factor\n",
    "    \n",
    "    vif_list = []\n",
    "    for i in range(df1.shape[1]):\n",
    "        vif = variance_inflation_factor(df1.to_numpy(),i)\n",
    "        vif_list.append(vif)\n",
    "        \n",
    "s1 = pd.Series(vif_list, index = df1.columns)     # converted into series\n",
    "s1.sort_values().plot(kind - 'barh')              # displaying by using 'barh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d489326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef9df84d",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y variable for model training\n",
    "\n",
    "x >> independent variables >> 2-D data   2d-array,df\n",
    "y >> dependent variable >> 1-D Series,array\n",
    "\n",
    "x = df.drop('Price', axis = 1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a469b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula for model training\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82161a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Linear Regression\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(x_train, y_train) # Gradient Descent >> best m and c values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c05765",
   "metadata": {},
   "source": [
    "#### Model Evaluation on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ae421",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation on Testing Data\n",
    "\n",
    "y_pred = linear_reg_model.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error :\",rmse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error :\",mae)\n",
    "\n",
    "r_squared_value = r2_score(y_test, y_pred)\n",
    "print(\"R Squared Value :\",r_squared_value)\n",
    "\n",
    "r2 = linear_reg_model.score(x_test, y_test) # Without Predict Function\n",
    "print(\"R2 :\",r2)\n",
    "\n",
    "adj_r2 = 1 - (((1-r_squared_value) * (x_test.shape[0] - 1))/(x_test.shape[0]- x_test.shape[1] - 1))\n",
    "print(\"Adjusted R-Squared Value :\",adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e24ed",
   "metadata": {},
   "source": [
    "#### Model Evaluation on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46649d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation on Training Data\n",
    "\n",
    "y_pred_train = linear_reg_model.predict(x_train) # 404 rows\n",
    "\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error :\",rmse)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "print(\"Mean Absolute Error :\",mae)\n",
    "\n",
    "r_squared_value = r2_score(y_train, y_pred_train)\n",
    "print(\"R Squared Value :\",r_squared_value)\n",
    "\n",
    "adj_r2 = 1 - (((1-r_squared_value) * (x_train.shape[0] - 1))/(x_train.shape[0]- x_train.shape[1] - 1))\n",
    "print(\"Adjusted R-Squared Value :\",adj_r2)\n",
    "\n",
    "r2 = linear_reg_model.score(x_train, y_train)\n",
    "print(\"R2 :\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd03f0c",
   "metadata": {},
   "source": [
    "#### Evaluate R-Squared Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f315332",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_list = []\n",
    "adj_r2_list = []\n",
    "for i in range(1, boston_df.shape[1]):\n",
    "    x = boston_df.iloc[:,:i]\n",
    "    model = LinearRegression()\n",
    "    model.fit(x,y)\n",
    "    r2 = model.score(x,y) # coef. of determination\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "    adj_r2 = 1 - (((1-r2) * (x.shape[0] - 1))/(x.shape[0]- x.shape[1] - 1))\n",
    "    adj_r2_list.append(adj_r2)\n",
    "    \n",
    "result_df = pd.DataFrame()\n",
    "result_df['Columns'] = x.columns\n",
    "result_df['R2 Values'] = r2_list\n",
    "result_df['Adj R2 Values'] = adj_r2_list\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d988f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "After train our model we need to check 3.Normality of Residual and 4. Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Normality of Residual:\n",
    "    Residuals should be normally distributed\n",
    "\n",
    "        1. kdeplot\n",
    "        2. Hypothesis Testing:\n",
    "            1. Shapiro\n",
    "            2. KsTest\n",
    "            3. normaltest\n",
    "\n",
    "        3. QQ-Plot\n",
    "        4. Skewness of Residual  (skew = 0 >> Data is normally distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Residual >> Ya - Yp   # find out residual\n",
    "\n",
    "Residual = y_train - y_pred_train\n",
    "\n",
    "# display residula by using kdeplot\n",
    "sns.kdeplot(Residual,fill= True)\n",
    "\n",
    "# display residula by using displot\n",
    "sns.distplot(Residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd68b92c",
   "metadata": {},
   "source": [
    "#### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive SKewness >> Tail of distribution is longer towards right hand side\n",
    "Negative Skewness >> Tail of distribution is longer towards left hand side\n",
    "Symmetrical data \n",
    "\n",
    "-0.5 to +0.5 >> Symmetrical Distribution (-0.4, 0, 0.1,0.3)\n",
    "-1 to -0.5 >> Negatively Skewed Distribution (-0.7,-0.90)\n",
    "skew < -1  >> Highly Negatively skewed Distribution(-1.1,-1.4,-1.15)\n",
    "\n",
    "+0.5 to 1.0 >> Positively SKewed distribution  (+0.6, +0.8)\n",
    "skew > +1  >> Highly Positively skewed Distribution(+1.1,+1.4,+1.15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
